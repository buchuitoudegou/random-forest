{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "from random import randrange\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000003\n",
      "10000003\n",
      "10000003\n",
      "10000003\n",
      "10000003\n"
     ]
    }
   ],
   "source": [
    "# read all data from train\n",
    "train_data = []\n",
    "train_label = []\n",
    "for i in range(1, 6):\n",
    "    tmp = pd.read_csv('./data/train%d.csv' % i, header=None)\n",
    "    label = pd.read_csv('./data/label%d.csv' % i, header=None)\n",
    "    train_data.append(tmp)\n",
    "    train_label.append(label)\n",
    "x = train_data[0]\n",
    "y = train_label[0]\n",
    "for i in range(4):\n",
    "    x = np.vstack((x, train_data[i]))\n",
    "    y = np.vstack((y, train_label[i]))\n",
    "dataset = np.hstack((x, y))\n",
    "kf = KFold(n_splits=5)\n",
    "k_fold_data = []\n",
    "for tra_idx, val_idx in kf.split(dataset):\n",
    "    tra = dataset[tra_idx]\n",
    "    val = dataset[val_idx]\n",
    "    k_fold_data.append((tra, val))\n",
    "    print(len(tra) + len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(index, value, dataset):\n",
    "    \"\"\"\n",
    "    description: split the current dataset by the `index`th feature and value\n",
    "    \"\"\"\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "def gini_index(groups, class_values):\n",
    "    \"\"\"\n",
    "    description: evaluate the gini value of the current groups, \n",
    "    containing the left group and the right group, splited by the nth feature.\n",
    "    `class_values` are all the possible values in the label column.\n",
    "    \"\"\"\n",
    "    gini = 0.0\n",
    "    for class_value in class_values:\n",
    "        for group in groups:\n",
    "            if len(group) == 0:\n",
    "                continue\n",
    "            proportion = [row[-1] for row in group].count(class_value) / len(group)\n",
    "            gini += (proportion * (1.0 - proportion))\n",
    "    return gini\n",
    "\n",
    "def to_terminal(group):\n",
    "    \"\"\"\n",
    "    description: terminate the split and set the most frequent value as the output\n",
    "    \"\"\"\n",
    "    output = [row[-1] for row in group]\n",
    "    return max(set(output), key=output.count)\n",
    "\n",
    "def get_split(dataset, n_features):\n",
    "    \"\"\"\n",
    "    description: choose a feature, which gains the highest gini value,\n",
    "    to split the dataset.\n",
    "    \"\"\"\n",
    "    class_values = [row[-1] for row in dataset]\n",
    "    s_index, s_value, s_score, s_groups = 999999, 999999, 999999, None\n",
    "    cur_features = [] # choose partition feature randomly\n",
    "    while len(cur_features) < log(2, n_features):\n",
    "        index = randrange(len(dataset[0] - 1))\n",
    "        if index not in cur_features:\n",
    "            cur_features.append(index)\n",
    "    for index in cur_features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < s_score:\n",
    "                s_index, s_value, s_score, s_groups = index, row[index], gini, groups\n",
    "    return { 'index': s_index, 'value': s_value, 'group': s_groups }\n",
    "\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    \"\"\"\n",
    "    description: split the node recursively.\n",
    "    \"\"\"\n",
    "    left, right = node['group']\n",
    "    del(node['group'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth + 1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth + 1)\n",
    "\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    \"\"\"\n",
    "    description: build a tree\n",
    "    \"\"\"\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "def predict(node, row):\n",
    "    \"\"\"\n",
    "    description: make prediction with the tree, navigating it and get the output\n",
    "    \"\"\"\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "def sub_sample(dataset, ratio):\n",
    "    \"\"\"\n",
    "    description: select the data ramdomly.\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = []\n",
    "    for i in range(n_trees):\n",
    "        sample = sub_sample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        print('finish')\n",
    "        trees.append(tree)\n",
    "    all_predict = []\n",
    "    all_y = []\n",
    "    for row in test:\n",
    "        predictions = [predict(tree, row) for tree in trees]\n",
    "        cur_predict = max(set(predictions), key=predictions.count)\n",
    "        all_y.append(row[-1])\n",
    "        all_predict.append(cur_predict)\n",
    "    return all_predict, r2_score(all_y, all_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n",
      "-0.3049475129155783\n",
      "finish\n",
      "-0.2787080208555386\n",
      "finish\n",
      "-0.0502273329390579\n",
      "finish\n",
      "-0.16307483454946547\n",
      "finish\n",
      "-0.060151858728111574\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    (train, test) = k_fold_data[i]\n",
    "    # train = train[:10000]\n",
    "    # test = test[:100]\n",
    "    _, score = random_forest(train, test, 5, 1, 0.8, 1, train[0].shape[0] - 1)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from multiprocessing import Lock, Queue, Pool\n",
    "lock = Lock()\n",
    "result = Queue()\n",
    "def append_tree(sample, max_depth, min_size, n_features):\n",
    "    tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "    result.put(tree)   \n",
    "p = Pool()\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "\n",
    "def multi_process_random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = []\n",
    "    for i in range(n_trees):\n",
    "        sample = sub_sample(train, sample_size)\n",
    "        p.apply_async(build_tree, args=(sample, max_depth, min_size, n_features,))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    trees = []\n",
    "    while not result.empty():\n",
    "        trees.append(result.get())\n",
    "    all_predict = []\n",
    "    all_y = []\n",
    "    for row in test:\n",
    "        predictions = [predict(tree, row) for tree in trees]\n",
    "        cur_predict = max(set(predictions), key=predictions.count)\n",
    "        all_y.append(row[-1])\n",
    "        all_predict.append(cur_predict)\n",
    "    return all_predict, r2_score(all_y, all_predict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "“pytorch”",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
